{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angelo Kelvakis\n",
    "### Predicting Sexual Violence: Unconsenting Media  Google Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was used to generate missing release year data and generate overview data of the media items.\n",
    "NOTE: In order to run properly, javascript must be disabled on your browser in order to make html parsable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Unconsenting Media found HERE: https://www.unconsentingmedia.org/list\n",
    "UM_Data = pd.read_csv('Data.csv')\n",
    "UM_Data = UM_Data.drop(['cleanName','cleanNameArticles','altName','comment','author','timeInFilm','posterUrl','youTubeUrl','createdAt','updatedAt'], axis=1)\n",
    "# replace all 0s in the 'yearOfRelease' column with NaNs\n",
    "UM_Data = UM_Data.replace(0,np.NaN)\n",
    "# create subsetted df with missing years\n",
    "Years_sv = UM_Data[UM_Data['yearOfRelease'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Google for Release Year\n",
    "CAUTION: Estimated run time = 4.5 hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = Years_sv\n",
    "\n",
    "url_main = \"https://www.google.com/search?q=\"\n",
    "\n",
    "leftover_data = []\n",
    "for i in df_movies['name']:\n",
    "    # Append the url string with the name of the movie\n",
    "    url_bucket = []\n",
    "    # Remove any punctuation from the name of the media\n",
    "    s = re.sub(r'[^\\w\\s]', '', i)\n",
    "    for w in s.split():\n",
    "        url_bucket.append(w)\n",
    "        url_bucket.append(\"+\")\n",
    "    # get published date for books\n",
    "    if df_movies[df_movies['name']==i]['itemType'].iloc[0] == 'book':\n",
    "        url_bucket_m = url_main + ''.join(url_bucket)+'Published'\n",
    "        flag = 'book'\n",
    "    if df_movies[df_movies['name']==i]['itemType'].iloc[0] == 'movie':\n",
    "        url_bucket_m = url_main + ''.join(url_bucket)+'movie+release+date'\n",
    "        flag = 'movie'\n",
    "    if df_movies[df_movies['name']==i]['itemType'].iloc[0] == 'TV show':\n",
    "        url_bucket_m = url_main + ''.join(url_bucket)+'TV+Show+release+date'\n",
    "        flag = 'TV show'\n",
    "    # obtain html\n",
    "    page = requests.get(url_bucket_m)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # find the description from the page\n",
    "    data = soup.findAll(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "    # troubleshoot misclassified movies\n",
    "    if len(data) == 0:\n",
    "        # misclassified tv show\n",
    "        if flag == 'movie':\n",
    "            url_bucket_m = url_main + ''.join(url_bucket)+'TV+Show+release+date'\n",
    "            page = requests.get(url_bucket_m)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            data = soup.findAll(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "            # if release date not found, make NA\n",
    "            if len(data) == 0:\n",
    "                Year = None\n",
    "                leftover_data.append([i,Year])\n",
    "                print(i,\"--\",Year)\n",
    "                continue\n",
    "        # misclassified movie\n",
    "        if flag == 'TV show':\n",
    "            url_bucket_m = url_main + ''.join(url_bucket)+'movie+release+date'\n",
    "            page = requests.get(url_bucket_m)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            data = soup.findAll(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "            # if release date not found, make NA\n",
    "            if len(data) == 0:\n",
    "                Year = None\n",
    "                leftover_data.append([i,Year])\n",
    "                print(i,\"--\",Year)\n",
    "                continue\n",
    "        # missclassified book (try TV show, then movie)\n",
    "        if flag == 'book':\n",
    "            url_bucket_m = url_main + ''.join(url_bucket)+'TV+Show+release+date'\n",
    "            page = requests.get(url_bucket_m)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            data = soup.findAll(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "            if len(data) == 0:\n",
    "                url_bucket_m = url_main + ''.join(url_bucket)+'movie+release+date'\n",
    "                page = requests.get(url_bucket_m)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                data = soup.findAll(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "                # if release date not found, make NA\n",
    "                if len(data) == 0:\n",
    "                    Year = None\n",
    "                    leftover_data.append([i,Year])\n",
    "                    print(i,\"--\",Year)\n",
    "                    continue\n",
    "    Year = data[0].text\n",
    "\n",
    "    leftover_data.append([i,Year])\n",
    "    print(i,\"--\",Year)\n",
    "\n",
    "df = pd.DataFrame(leftover_data,columns =['Title', 'Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data & Rebind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN rows without a Year\n",
    "df2=df.dropna(axis=0)\n",
    "df2.shape\n",
    "# Convert 'Year' column into just year\n",
    "df2['Year'] = [re.search(r'[12]\\d{3}', y).group(0) for y in df2['Year']]\n",
    "\n",
    "# Rebind the Year data to redo_sv where the title matches\n",
    "df2 = df2.rename(columns = {'Title':'name','Year':'yearOfRelease'})\n",
    "# bind data and leave Nas\n",
    "df3 = pd.merge(redo_sv, df2, on=\"name\",how=\"outer\")\n",
    "# Fill those NAs with data from binded column\n",
    "df3['yearOfRelease_x'] = df3['yearOfRelease_x'].fillna(df3['yearOfRelease_y'])\n",
    "# Fix column names and drop extra column\n",
    "df3 = df3.drop(['yearOfRelease_y'], axis=1)\n",
    "df3 = df3.rename(columns = {'yearOfRelease_x':'yearOfRelease'})\n",
    "\n",
    "# Drop rows with remainder of NAs in Year column\n",
    "df3=df3.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Google for overview data\n",
    "CAUTION: Estimated runtime 4 hrs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find missing overviews \n",
    "missing_over = df3[df3['overview'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = missing_over\n",
    "\n",
    "url_main = \"https://www.google.com/search?q=\"\n",
    "\n",
    "leftover_data = []\n",
    "for i in df_movies['title']:\n",
    "    # Append the url string with the name of the movie\n",
    "    url_bucket = []\n",
    "    # Remove any punctuation from the name of the media\n",
    "    s = re.sub(r'[^\\w\\s]', '', i)\n",
    "    for w in s.split():\n",
    "        url_bucket.append(w)\n",
    "        url_bucket.append(\"+\")\n",
    "    # get published date for books\n",
    "    if df_movies[df_movies['title']==i]['itemType'].iloc[0] == 'book':\n",
    "        url_bucket_m = url_main + ''.join(url_bucket)+str(int(df_movies[df_movies['title']==i]['yearOfRelease'].iloc[0]))+'+book+Synopsis'\n",
    "        flag = 'book'\n",
    "        # obtain html\n",
    "        page = requests.get(url_bucket_m)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        # find the description from the page\n",
    "        data = soup.findAll(\"div\", class_=\"BNeawe s3v9rd AP7Wnd\")\n",
    "        \n",
    "    if df_movies[df_movies['title']==i]['itemType'].iloc[0] == 'movie':\n",
    "        url_bucket_m = url_main + ''.join(url_bucket)+str(int(df_movies[df_movies['title']==i]['yearOfRelease'].iloc[0]))+'+movie+overview'\n",
    "        flag = 'movie'\n",
    "        # obtain html\n",
    "        page = requests.get(url_bucket_m)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        # find the description from the page\n",
    "        data = soup.findAll(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "        \n",
    "    if df_movies[df_movies['title']==i]['itemType'].iloc[0] == 'TV show':\n",
    "        url_bucket_m = url_main + ''.join(url_bucket)+str(int(df_movies[df_movies['title']==i]['yearOfRelease'].iloc[0]))+'+TV+Show+Synopsis'\n",
    "        flag = 'TV show'\n",
    "        # obtain html\n",
    "        page = requests.get(url_bucket_m)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        # find the description from the page\n",
    "        data = soup.findAll(\"div\", class_=\"BNeawe s3v9rd AP7Wnd\")\n",
    "    \n",
    "    # troubleshoot misclassified media\n",
    "    if len(data) == 0:\n",
    "        # misclassified tv show\n",
    "        if flag == 'movie':\n",
    "            url_bucket_m = url_main + ''.join(url_bucket)+str(int(df_movies[df_movies['title']==i]['yearOfRelease'].iloc[0]))+'+TV+Show+summary'\n",
    "            page = requests.get(url_bucket_m)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            data = soup.findAll(\"div\", class_=\"BNeawe s3v9rd AP7Wnd\")\n",
    "            # if release date not found, make NA\n",
    "            if len(data) == 0:\n",
    "                Overview = None\n",
    "                leftover_data.append([i,Overview])\n",
    "                print(i,\"--\",Overview)\n",
    "                continue\n",
    "        # misclassified movie\n",
    "        if flag == 'TV show':\n",
    "            url_bucket_m = url_main + ''.join(url_bucket)+str(int(df_movies[df_movies['title']==i]['yearOfRelease'].iloc[0]))+'+movie+overview'\n",
    "            page = requests.get(url_bucket_m)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            data = soup.findAll(\"div\", class_=\"BNeawe iBp4i AP7Wnd\")\n",
    "            # if release date not found, make NA\n",
    "            if len(data) == 0:\n",
    "                Overview = None\n",
    "                leftover_data.append([i,Overview])\n",
    "                print(i,\"--\",Overview)\n",
    "                continue\n",
    "        # missclassified book (try TV show, then movie)\n",
    "        if flag == 'book':\n",
    "            url_bucket_m = url_main + ''.join(url_bucket)+str(int(df_movies[df_movies['title']==i]['yearOfRelease'].iloc[0]))+'+TV+Show+summary'\n",
    "            page = requests.get(url_bucket_m)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            data = soup.findAll(\"div\", class_=\"BNeawe s3v9rd AP7Wnd\")\n",
    "            if len(data) == 0:\n",
    "                url_bucket_m = url_main + ''.join(url_bucket)+str(int(df_movies[df_movies['title']==i]['yearOfRelease'].iloc[0]))+'+movie+overview'\n",
    "                page = requests.get(url_bucket_m)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                data = soup.findAll(\"div\", class_=\"BNeawe s3v9rd AP7Wnd\")\n",
    "                # if release date not found, make NA\n",
    "                if len(data) == 0:\n",
    "                    Overview = None\n",
    "                    leftover_data.append([i,Overview])\n",
    "                    print(i,\"--\",Overview)\n",
    "                    continue\n",
    "    # Pull out first response                \n",
    "    Overview = data[0].text\n",
    "    \n",
    "    # deal with IMBD data\n",
    "    if '·' in Overview:\n",
    "        imbdlist = data[0].text.split('\\n')\n",
    "        Overview = max(imbdlist,key=len)\n",
    "        \n",
    "    leftover_data.append([i,Overview])\n",
    "    print(i,\"--\",Overview)\n",
    "    #print(url_bucket_m)\n",
    "\n",
    "df = pd.DataFrame(leftover_data,columns =['title', 'overview'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data and Rebind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind data back to df3 to fill in NAs\n",
    "final_df = pd.merge(df3, df, on=\"title\",how=\"outer\")\n",
    "final_df['overview_x'] = final_df['overview_x'].fillna(final_df['overview_y'])\n",
    "# Fix column names and drop extra column\n",
    "final_df = final_df.drop(['overview_y'], axis=1)\n",
    "final_df = final_df.rename(columns = {'overview_x':'overview'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('REDO_Overview_txtAnalysis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
